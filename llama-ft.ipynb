{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9846403,"sourceType":"datasetVersion","datasetId":6041306}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers bitsandbytes peft datasets evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForSeq2Seq\nfrom datasets import Dataset\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/sentiment/sentiment_data.csv\")\ndataset = Dataset.from_pandas(df)\n\n# Split dataset\ndataset = dataset.train_test_split(test_size=0.2)\n\n# Model and tokenizer loading\nbase_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n\ndevice_map = {\n    \"model\": \"auto\",  # Automatically assign layers to GPU if possible\n    \"quantizer\": \"cpu\"  # Offload the quantizer to the CPU if GPU memory is insufficient\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    device_map=device_map,  # Use the custom device map\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    load_in_8bit_fp32_cpu_offload=True  # Enable CPU offloading in FP32\n)\n\nmodel.config.use_cache = False\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Tokenization and Data Collation\ndef tokenize_function(examples):\n    return tokenizer(examples['Text'], padding='max_length', truncation=True, max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n\n# Data Collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\")\n\n# LoRA Config\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Prepare model for training and apply PEFT\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-fine-tuned\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,  \n    gradient_accumulation_steps=8,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\"\n)\n\n# Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluation\neval_results = trainer.evaluate()\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}